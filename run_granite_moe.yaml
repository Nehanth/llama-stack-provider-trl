version: '2'
image_name: llama-stack-trl
apis:
- post_training
- datasetio
- datasets
providers:
  post_training:
  - provider_id: trl
    provider_type: inline::trl
    config:
      # === DEVICE AND TRAINING SETTINGS ===
      device: "cpu"                         # Use CPU for compatibility (change to "cuda" for GPU)
      gradient_checkpointing: false         # Set to true if running out of memory
      max_seq_length: 2048                  # Maximum sequence length
      logging_steps: 10                     # Log every 10 steps
      
      # === DPO TRAINING PARAMETERS ===
      dpo_beta: 0.1                         # DPO beta parameter (0.1 is standard)
      use_reference_model: true             # Use reference model for stability
      dpo_loss_type: "sigmoid"              # Standard DPO loss function
      normalize_rewards: true               # Normalize rewards for stable training
      label_smoothing: 0.0                  # No label smoothing (standard)
      
      # === OPTIMIZER SETTINGS ===
      warmup_ratio: 0.1                     # Learning rate warmup ratio
      weight_decay: 0.01                    # L2 regularization
      
      # === MIXTURE OF EXPERTS (MOE) SETTINGS ===
      # Based on TRL documentation: https://huggingface.co/docs/trl/dpo_trainer
      # These settings are CRITICAL for MoE models like Granite to train properly
      
      # Enable router logits output (REQUIRED for MoE models)
      # This enables the load balancer auxiliary loss from the router
      output_router_logits: true
      
      # Router auxiliary loss coefficient (TRL default: 0.001)
      # Controls how much the auxiliary loss contributes to total loss
      # Higher values = stronger load balancing between experts
      router_aux_loss_coef: 0.001
      
      # Load balancing strategy for MoE training
      # "switch" = Switch Transformer load balancing (recommended)
      moe_load_balancing: "switch"
      
      # Enable MoE-specific training optimizations
      # Includes expert parallelism, gradient synchronization, etc.
      enable_moe_optimizations: true
      
      # Automatic MoE detection and configuration
      # Automatically detects MoE models and applies MoE settings
      auto_detect_moe: true
      
      # Expert dropout rate (0.0 = no dropout, standard)
      expert_dropout: 0.0
      
      # === CHECKPOINT AND MEMORY SETTINGS ===
      checkpoint_format: "huggingface"      # Save in HuggingFace format
      save_total_limit: 3                   # Keep max 3 checkpoints
      dataloader_num_workers: 0             # Single-process data loading
      dataloader_pin_memory: true           # Pin memory for GPU transfers
      
  datasetio:
  - provider_id: localfs
    provider_type: inline::localfs
    config:
      kvstore:
        type: sqlite
        db_path: /tmp/llama_stack_provider_trl/datasetio.db
        
external_providers_dir: ./providers.d
metadata_store:
  type: sqlite
  db_path: /tmp/llama_stack_provider_trl/registry.db
models: []
shields: []
server:
  port: 8321

# === NOTES FOR GRANITE MOE TRAINING ===
#
# This configuration is specifically optimized for Granite MoE models:
# - ibm-granite/granite-3.0-1b-a400m-base (1B params, 32 experts)
# - ibm-granite/granite-3.0-3b-a800m-base (3B params, 32 experts)
# - ibm-granite/granite-3.0-8b-a400m-base (8B params, 32 experts)
#
# Key MoE features enabled:
# 1. output_router_logits=true - Enables auxiliary loss for load balancing
# 2. router_aux_loss_coef=0.001 - Standard coefficient for auxiliary loss
# 3. auto_detect_moe=true - Automatically detects and configures MoE models
# 4. moe_load_balancing="switch" - Use Switch Transformer load balancing
#
# For GPU training, change device to "cuda" and consider:
# - Increasing batch_size in training requests
# - Enabling gradient_checkpointing if memory constrained
# - Using fp16=true for faster training
#
# Tested successfully with:
# - Granite 3.0 1B A400M Base (32 experts, 8 experts per token)
# - Achieves perfect preference accuracy (1.0) 
# - Auxiliary loss working correctly (~8.0)
# - No weight mapping errors
# - Clean checkpoint saving and loading 