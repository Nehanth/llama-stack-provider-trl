version: '2'
image_name: trl-post-training-fsdp
apis:
- post_training
- datasetio
providers:
  post_training:
  - provider_id: trl_fsdp
    provider_type: remote::trl_fsdp
    config:
      # Remote service configuration
      service_config:
        host: "localhost"
        port: 8322
        request_timeout: 3600  # 1 hour for long training jobs
        max_concurrent_jobs: 1
      
      # FSDP distributed training configuration
      fsdp_config:
        sharding_strategy: "FULL_SHARD"  # Most memory efficient
        mixed_precision_policy: "bf16"   # Use bfloat16 for better performance
        cpu_offload: false               # Set to true for larger models
        limit_all_gathers: true
        forward_prefetch: true
        backward_prefetch: "BACKWARD_PRE"
      
      # torch.distributed configuration
      torch_run_config:
        nnodes: 1                        # Single node (machine)
        nproc_per_node: 2                # Number of GPUs to use
        node_rank: 0
        master_addr: "localhost"
        master_port: 29500
        max_restarts: 0
        rdzv_timeout: 1800               # 30 minutes
      
      # Training configuration optimized for FSDP
      device: "cuda"
      max_seq_length: 4096               # Longer sequences possible with FSDP
      gradient_checkpointing: true       # Important for memory efficiency
      checkpoint_format: "fsdp"          # FSDP-optimized checkpoints
      use_fsdp_state_dict: true
      
      # DPO parameters
      dpo_beta: 0.1
      use_reference_model: true
      dpo_loss_type: "sigmoid"
      
      # Performance optimization
      activation_checkpointing: "selective"
      compile_model: false               # Set to true for PyTorch 2.0+
      dataloader_num_workers: 4
      dataloader_pin_memory: true
      dataloader_persistent_workers: true
      
      # Logging configuration
      log_on_each_node: false            # Only log on rank 0
      logging_steps: 10
      save_steps: 100
      save_total_limit: 3

  datasetio:
  - provider_id: localfs
    provider_type: inline::localfs
    config:
      kvstore:
        type: sqlite
        db_path: /tmp/llama_stack_provider_trl_fsdp/datasetio.db

external_providers_dir: ./providers.d
metadata_store:
  type: sqlite
  db_path: /tmp/llama_stack_provider_trl_fsdp/registry.db
models: []
shields: []
server:
  port: 8321  # Main Llama Stack port (remote provider runs on 8322) 